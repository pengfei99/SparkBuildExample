# Build spark submit jars with sbt

In this tutorial, we will use sbt to build a spark submit jar file  

## 1. Prerequisites

You need to install and configure
- java(jdk 11 or later)
- scala(2.12 or later)
- sbt(1.9 or later)


Below is my project setup

```shell
$ java -version
openjdk version "11.0.2" 2019-01-15
OpenJDK Runtime Environment 18.9 (build 11.0.2+9)
OpenJDK 64-Bit Server VM 18.9 (build 11.0.2+9, mixed mode)

$ scala -version
Scala code runner version 2.12.19 -- Copyright 2002-2024, LAMP/EPFL and Lightbend, Inc.


$ sbt -version
sbt version in this project: 1.10.0
sbt script version: 1.10.0

```

## 2. Build the project skeleton

Suppose we want to create a spark job which will count word of a list of files.

### 2.1 Build the basic project layout

The project name is `WordCountSbt`

```shell
mkdir WordCountSbt

cd WordCountSbt
mkdir -p src/main/scala
mkdir -p src/test/scala
```

### 2.2. Build the module layout

Suppose your organization name is `example`, you can build your module like below

```shell
mkdir -p src/main/scala/com/example
mkdir -p src/test/scala/com/example
```

### 2.3 Build the package

I call the package `wordcount`, you can call your package as you want

```shell
mkdir -p src/main/scala/com/example/wordcount
mkdir -p src/test/scala/com/example/wordcount
```

### 2.4 Create the build.sbt file for the project WordCountSbt

The `build.sbt` file is the main config of your sbt project. It must be located under the root path of `WordCount`.
It defines:
1. define the version of the project build
2. define the scala version
3. define the organization
4. define the project name

Below is an example.
```shell
ThisBuild / version := "0.1.0-SNAPSHOT"

ThisBuild / scalaVersion := "2.12.19"
ThisBuild / organization := "org.casd"

lazy val root = (project in file("."))
  .settings(
    name := "WordCount"

  )

```

> This is the minimum setup for a sbt project. We will enrich it to add spark and build jars after.
>

### 2.5 Valid the project structure

If everything went well, we should have a valid project structure which allow us to run some basic scala code.

Create a scala object with the name `WordCountMain.scala`, add put the below code.

```scala
package org.casd.wordcount

object WordCountMain extends App {
  println("hello world")
}
```
If you can run the code and see the output, we can go to section 3. Otherwise, you need to debug

### 2.6 Use sbt command

If you don't have IDE, you can use the sbt command line to run your project

```shell
cd path/to/WordCountSbt

# start a sbt shell
sbt

# run the project
run

# exit the sbt shell
exit
```

### 3. Add spark dependencies

To use spark in this project, we need to add spark as dependencies, add the following lines in the `build.sbt`

```shell
val sparkVersion = "3.5.1"
libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % sparkVersion % "provided",
  "org.apache.spark" %% "spark-sql" % sparkVersion % "provided"
)

fork := true
```
The `libraryDependencies ++= Seq()` specifies all dependencies that the project requires. In this example, we need
- spark-core
- spark-sql

The `fork := true` is required to run the project from the command line, It means `run the code in a new instance of JVM`.
Without it, the program ends on an exception.

More details can be found in this post [why fork:=true](https://stackoverflow.com/questions/44298847/why-do-we-need-to-add-fork-in-run-true-when-running-spark-sbt-application?source=post_page-----80e2680d3528--------------------------------)

#### 3.1 Add the spark sample code

To build the jar file, we need to add a spark simple code. Put the below code in a file called `WordCountMain`

```scala
package org.casd.wordcount

import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.functions.{col, count}
import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}

import scala.sys.exit



object WordCountMain extends App {
  /* This method create a spark session */
  def createSparkSession(appName: String, isLocal: Boolean): SparkSession = {
    if (isLocal) {
      SparkSession
        .builder()
        .config("spark.sql.caseSensitive", value = true)
        .config("spark.sql.session.timeZone", value = "UTC")
        .config("spark.driver.memory", value = "8G")
        .appName(appName)
        .master("local[*]")
        .getOrCreate()
    } else {
      SparkSession
        .builder()
        .config("spark.sql.caseSensitive", value = true)
        .config("spark.sql.session.timeZone", value = "UTC")
        .appName(appName)
        .getOrCreate()
    }
  }
 /* This method do the word count of a file*/
  def countWord(data: RDD[(String)]):RDD[Row]  = {
    val result:RDD[(String, Int)] = data.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey((a,b)=>a+b)
    val resInRow:RDD[Row] = result.map(t => Row(t._1,t._2))
    resInRow
  }

  /* This method parse the command line arguments*/
  def parseArgs(args: Array[String]): (String, String) = {
    val usage = """
     Usage: spark-submit spark-wordcount.jar [--input-path path] [--output-path path]
     """
    if (args.isEmpty || args.length % 2 != 0) {
      println(usage)
      exit(1)
    }
    var inPath:String=""
    var outPath:String=""
    args.sliding(2, 2).toList.collect {
      case Array("--input-path", arg1: String) => inPath = arg1
      case Array("--output-path", arg2: String) => outPath = arg2
    }

    println(s"input path: ${inPath}")
    println(s"output path: ${outPath}")

    (inPath, outPath)
  }

  // step1: create a spark session, set local to true for testing, set it to False when building the jar
  val spark: SparkSession = createSparkSession("Word count", isLocal = false)

  // step2: parse argument
  val (inputPath, outputPath) = parseArgs(args = args)
  // val inputPath = "/home/pengfei/git/SparkBuildExample/data"
  // val outputPath = "/tmp"

  // step3: read data
  val sc: SparkContext = spark.sparkContext
  val filePath: String = s"${inputPath}/*.txt"
  val data: RDD[String] = sc.textFile(filePath)

  // step4: get word count rdd
  val countInRdd: RDD[Row] = countWord(data)

  // step5: create dataframe from the rdd
  val schema: StructType = new StructType().add(StructField("Word", StringType, nullable = false)).add(StructField("Count", IntegerType, nullable = false))
  val df: DataFrame = spark.createDataFrame(countInRdd, schema)
  df.show(10)

  // step6: write dataframe to output path
  val outfilePath = s"${outputPath}/wordCountRes"
  df.coalesce(1).write.mode("overwrite").csv(outfilePath)
}


```

To run the sample code, you can use the below sbt command

```shell
cd path/to/WordCountSbt

sbt "run --input-path /home/pengfei/git/SparkBuildExample/data --output-path /tmp"
```

If everything works well you should see the output csv files stores the result dataframe

### 4. Build the jar file

Now we are ready to build the jar file for the spark submit.

We need to do:
1. Add the **sbt-assembly** plugin into file `plugins.sbt`
2. Add `assemblyMergeStrategy` into file `build.sbt`
3. Remove the spark local session mode
4. Remove spark-core, spark-sql from the jar 

#### 4.1 sbt-assembly plugin

This plugin can help you to add all project required dependencies into the same jar file of your project. This can avoid
`class not found exceptions` when you do spark submit

More details about the **sbt-assembly** plugin can be found [here](https://github.com/sbt/sbt-assembly#merge-strategy)

In the `path/to/WordCountSbt/project` folder create a file `plugins.sbt`, then put the below code 

```shell
# I used the current latest version, you can check the github page to get the latest version
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "2.2.0")
```
#### 4.2 assemblyMergeStrategy

When building the jar file, you may encounter merge conflict between the dependencies jar, and the default strategy is
to stop the build and print error. As a result we need to handle the merge issues

You can read these two articles to know more about `assemblyMergeStrategy`
- [Introduction to creating Fat JAR using sbt.](https://www.baeldung.com/scala/sbt-fat-jar)
- [StackOverflow: assembling Spark jobs with good discussion.](https://stackoverflow.com/a/48061746)

#### 4.3 Remove the spark local session mode
In the `WordCountMain`, when I build the spark session, I put the option `isLocal=true`, you need to change it to false

The `SparkSession` will only contain the below options. All other options should be provided by the spark-submit command
line configuration

```shell
spark = SparkSession
        .builder()
        .config("spark.sql.caseSensitive", value = true)
        .config("spark.sql.session.timeZone", value = "UTC")
        .appName(appName)
        .getOrCreate()
```

#### 4.4 Remove spark-core, spark-sql from the jar

**Do not include the spark-core and spark-sql** jar into your assembly-jar, because if the jar are submitted to a 
spark cluster which use different version of spark, it will create conflicts. However, if there are additional 
dependencies, they need to be included in the fat JAR so that the code can use them

Add the keyword `provided` to exclude the dependencies from the assembly jar file. Below is an example

```shell
libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % sparkVersion % "provided",
  "org.apache.spark" %% "spark-sql" % sparkVersion % "provided"
)
```

> Please note that after marking spark-core and spark-sql as provided, the JAR cannot be run directly in local 
> mode due to the absence of these dependencies. To do so, remove provided, build the assembly again, and run.

#### 4.5 Build the assembly jar

Use the below command to build the assembly jar

```shell
cd path/to/WordCountSbt
sbt clean
sbt assembly
```

If you want to use a custom jar name you can add the below line into `build.sbt`.

```shell
assembly / assemblyJarName := "spark-wordcount.jar"
```
 
If everything works well, you will find the generated jar file in `path/to/WordCountSbt/target/scala-2.12` 
 
### 5. Submit the generated jar file

```shell
cd path/to/WordCountSbt/target/scala-2.12

spark-submit --class org.casd.wordcount.WordCountMain --master local spark-wordcount.jar --input-path /home/pengfei/git/SparkBuildExample/data --output-path /tmp
```

### 6. Custom log4j config

The default log level for spark is `INFO`, it's very good for debug, but not so good for production version.

You can tell spark to use your custom log configuration, for local mode, you can put the custom `log4j2.properties` file
in to `path/to/WordCountSbt/src/main/resources`

Below is an example of `log4j2.properties`

```shell
rootLogger.level = warn
rootLogger.appenderRef.stdout.ref = console

appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_OUT
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex
```

For cluster mode, you need to add the custom log config in to the spark-submit commands

```shell
cd path/to/WordCountSbt/target/scala-2.12

spark-submit --class org.casd.wordcount.WordCountMain \
             --files /home/pengfei/git/SparkBuildExample/WordCountSbt/src/main/resources/log4j2.properties \
             --conf "spark.driver.extraJavaOptions=-Dlog4j2.configurationFile=file:/home/pengfei/git/SparkBuildExample/WordCountSbt/src/main/resources/log4j2.properties" \
             --master local \
             spark-wordcount.jar --input-path /home/pengfei/git/SparkBuildExample/data --output-path /tmp
```